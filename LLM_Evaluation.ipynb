{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZKg4tvL2vGq4HyVsHEPlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcano0505/LLM_Evaluations/blob/main/LLM_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-VxeRxNDtERj",
        "outputId": "dfffbefb-0336-4846-9f6c-b9e41ff901b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Initialize Groq LLMs\n",
        "llm_answerer = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")\n",
        "llm_evaluator = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama-3.1-70b-versatile\")\n",
        "\n",
        "# Define prompts\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"Answer the following question:\n",
        "    {question}\"\"\",\n",
        ")\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"\"\"Evaluate the following answer to the question:\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Provide a score between 1 and 10, where 1 is the worst and 10 is the best.\n",
        "    Also provide a brief explanation for your score.\"\"\",\n",
        ")\n",
        "\n",
        "# Create chains (using Groq LLMs)\n",
        "answer_chain = LLMChain(llm=llm_answerer, prompt=answer_prompt)\n",
        "evaluation_chain = LLMChain(llm=llm_evaluator, prompt=evaluation_prompt)\n",
        "\n",
        "# Q&A Section\n",
        "question = input(\"Enter your question: \")\n",
        "answer = answer_chain.run(question)\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Evaluate the answer\n",
        "evaluation = evaluation_chain.run(question=question, answer=answer)\n",
        "print(f\"Evaluation: {evaluation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbaaAL4-bHLh",
        "outputId": "ec032e27-b076-4ce0-adf9-cc471eccd5f1"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What city in the US is growing the fastest?\n",
            "Answer: According to the United States Census Bureau, the fastest-growing city in the US is Austin, Texas. Between 2020 and 2021, Austin's population grew by 3.8%, making it the fastest-growing major city in the country. This growth is largely driven by the city's thriving tech industry, a strong job market, and a high quality of life.\n",
            "\n",
            "Other fast-growing cities in the US include:\n",
            "\n",
            "1. Orlando, Florida (3.6% growth rate)\n",
            "2. Phoenix, Arizona (3.4% growth rate)\n",
            "3. Tampa, Florida (3.3% growth rate)\n",
            "4. Raleigh-Durham, North Carolina (3.2% growth rate)\n",
            "5. Las Vegas, Nevada (3.1% growth rate)\n",
            "\n",
            "These cities are experiencing rapid growth due to a combination of factors, including a strong economy, affordable housing, and a desirable climate. However, it's worth noting that the growth rates can vary depending on the source and the time frame considered.\n",
            "Evaluation: I would give this answer a score of 9.\n",
            "\n",
            "The answer provides a clear and direct response to the question, citing a specific source (the United States Census Bureau) to support the claim that Austin, Texas is the fastest-growing city in the US. The answer also provides additional context and information about the drivers of growth in Austin, as well as a list of other fast-growing cities in the US. The inclusion of specific growth rates and a brief explanation of the factors contributing to growth in these cities adds depth and credibility to the answer. The only reason I wouldn't give it a perfect score is that the answer could benefit from a bit more nuance and discussion of potential challenges or drawbacks associated with rapid growth, but overall it is a well-informed and well-structured response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P78DjWQ1g8a",
        "outputId": "27853758-a1c4-413c-845d-0e29719b2ed6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ngrok\n",
            "Successfully installed ngrok-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "def main():\n",
        "  load_dotenv()\n",
        "  GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "  # Model options\n",
        "  MODEL_OPTIONS_LIST = [\n",
        "    \"llama-3.3-70b-versatile\",\n",
        "    \"mixtral-8x7b-32768\",\n",
        "    \"gemma2-9b-it\",\n",
        "    \"llama-3.1-8b-instant\",\n",
        "  ]\n",
        "  # --- Streamlit app ---\n",
        "  st.title(\"LLM Evaluation\")\n",
        "\n",
        "  # Model selection for answering\n",
        "  st.subheader(\"Select Model for Answering:\")\n",
        "  selected_answer_model = st.selectbox(\"Answering Model:\", MODEL_OPTIONS_LIST)\n",
        "\n",
        "  # Model selection for evaluating\n",
        "  st.subheader(\"Select Model for Evaluating:\")\n",
        "  selected_eval_model = st.selectbox(\"Evaluating Model:\", MODEL_OPTIONS_LIST)\n",
        "\n",
        "  llm_answerer = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=selected_answer_model)\n",
        "  llm_evaluator = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=selected_eval_model)\n",
        "\n",
        "  answer_prompt = PromptTemplate(\n",
        "      input_variables=[\"question\"],\n",
        "      template=\"\"\"Answer the following question:\n",
        "      {question}\"\"\",\n",
        "  )\n",
        "  evaluation_prompt = PromptTemplate(\n",
        "      input_variables=[\"question\", \"answer\"],\n",
        "      template=\"\"\"Evaluate the following answer to the question:\n",
        "      Question: {question}\n",
        "      Answer: {answer}\n",
        "\n",
        "      Provide a score between 1 and 10, where 1 is the worst and 10 is the best.\n",
        "      Also provide a brief explanation for your score.\"\"\",\n",
        "  )\n",
        "\n",
        "  answer_chain = LLMChain(llm=llm_answerer, prompt=answer_prompt)\n",
        "  evaluation_chain = LLMChain(llm=llm_evaluator, prompt=evaluation_prompt)\n",
        "\n",
        "\n",
        "  question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "  if question:\n",
        "      with st.spinner(\"Evaluating...\"):\n",
        "          answer = answer_chain.run(question)\n",
        "          evaluation = evaluation_chain.run(question=question, answer=answer)\n",
        "\n",
        "      st.subheader(\"Answer:\")\n",
        "      st.write(answer)\n",
        "\n",
        "      st.subheader(\"Evaluation:\")\n",
        "      st.write(evaluation)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At4utfsN1oAO",
        "outputId": "ca53ea81-8359-4cef-e316-cc69c0ac0364"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glzLjlQMX1fV",
        "outputId": "c1446b8d-b062-4f5b-a12e-0b51a06ed275"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.80.242.39\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.80.242.39:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://clear-falcons-shake.loca.lt\n",
            "/content/app.py:53: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  answer_chain = LLMChain(llm=llm_answerer, prompt=answer_prompt)\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "MODEL_OPTIONS_LIST = [\n",
        "    \"llama-3.3-70b-versatile\",\n",
        "    \"mixtral-8x7b-32768\",\n",
        "    \"gemma2-9b-it\",\n",
        "    \"llama-3.1-8b-instant\",\n",
        "]\n",
        "\n",
        "# Display model options to the user\n",
        "print(\"Available Models:\")\n",
        "for i, model_name in enumerate(MODEL_OPTIONS_LIST):\n",
        "    print(f\"{i + 1}. {model_name}\")\n",
        "\n",
        "# Get user's choice for the answer model\n",
        "while True:\n",
        "    try:\n",
        "        answer_choice = int(input(\"Enter the number of the model you want to use for answering: \"))\n",
        "        if 1 <= answer_choice <= len(MODEL_OPTIONS_LIST):\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number from the list.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# Get user's choice for the evaluation model\n",
        "while True:\n",
        "    try:\n",
        "        eval_choice = int(input(\"Enter the number of the model you want to use for evaluation: \"))\n",
        "        if 1 <= eval_choice <= len(MODEL_OPTIONS_LIST):\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number from the list.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# Get selected model names\n",
        "selected_answer_model = MODEL_OPTIONS_LIST[answer_choice - 1]  # Adjust for 0-based indexing\n",
        "selected_eval_model = MODEL_OPTIONS_LIST[eval_choice -1]\n",
        "\n",
        "llm_answerer = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=selected_answer_model)\n",
        "llm_evaluator = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=selected_eval_model)\n",
        "\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"Answer the following question:\n",
        "    {question}\"\"\",\n",
        ")\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"\"\"Evaluate the following answer to the question:\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Provide a score between 1 and 10, where 1 is the worst and 10 is the best.\n",
        "    Also provide an in-depth, but concise, explanation for your score so the user has a detailed reasoning behind the score.\"\"\",\n",
        ")\n",
        "\n",
        "answer_chain = LLMChain(llm=llm_answerer, prompt=answer_prompt)\n",
        "evaluation_chain = LLMChain(llm=llm_evaluator, prompt=evaluation_prompt)\n",
        "\n",
        "\n",
        "question = input(\"Enter your question: \")\n",
        "answer = answer_chain.run(question)\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Evaluate the answer\n",
        "evaluation = evaluation_chain.run(question=question, answer=answer)\n",
        "print(f\"Evaluation: {evaluation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrN1T3z5ilwi",
        "outputId": "7d15cfa1-6f9e-4979-b0af-5906dda59184"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Models:\n",
            "1. llama-3.3-70b-versatile\n",
            "2. mixtral-8x7b-32768\n",
            "3. gemma2-9b-it\n",
            "4. llama-3.1-8b-instant\n",
            "Enter the number of the model you want to use for answering: 3\n",
            "Enter the number of the model you want to use for evaluation: 2\n",
            "Enter your question: What are the biggest companies to expand hiring in tech 2025?\n",
            "Answer: I cannot predict the future, including which companies will expand hiring the most in tech by 2025. \n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **The tech landscape is constantly changing:** New companies emerge, existing ones merge or are acquired, and market trends shift rapidly.\n",
            "* **Economic factors play a huge role:** Hiring decisions are heavily influenced by economic conditions, which are impossible to predict with certainty. \n",
            "* **Internal company strategies are confidential:**  Companies' future hiring plans are not public knowledge.\n",
            "\n",
            "**However, I can give you some factors that might influence hiring trends in tech:**\n",
            "\n",
            "* **Growth areas:** Fields like artificial intelligence, cloud computing, cybersecurity, and data science are expected to continue growing, potentially leading to increased hiring in those areas.\n",
            "* **Emerging technologies:** New technologies like blockchain, the metaverse, and quantum computing could create new job opportunities.\n",
            "* **Global competition:** Companies expanding into new markets may increase hiring in those regions.\n",
            "* **Automation and AI:** While AI might automate some jobs, it will also create new roles focused on developing, implementing, and managing AI systems.\n",
            "\n",
            "\n",
            "To stay informed about hiring trends, I recommend following industry news, attending tech conferences, and networking with professionals in the field. \n",
            "\n",
            "\n",
            "Evaluation: I would give the answer a score of 9.\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The response accurately acknowledges the inability to predict specific companies' hiring plans due to the dynamic nature of the tech industry, confidential strategies, and unpredictable economic factors. These points demonstrate a clear understanding of the limitations and complexity involved in making such predictions.\n",
            "\n",
            "Furthermore, the answer provides valuable insights into possible factors that may influence hiring trends, such as growth areas, emerging technologies, global competition, and the impact of AI and automation. This information equips the reader with the knowledge to make informed decisions and predictions based on current trends.\n",
            "\n",
            "While the answer does not list specific companies, it still offers valuable insights and guidance on how to stay informed about future hiring trends. This approach is more helpful and reliable for the reader than speculating on specific companies without solid evidence or certainty.\n",
            "\n",
            "Overall, the answer is well-structured, informative, and honest, making it a high-quality response worthy of a score of 9.\n"
          ]
        }
      ]
    }
  ]
}